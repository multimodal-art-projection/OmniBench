<!DOCTYPE html>
<html lang="en">
  <head>
    <title>OmniBench</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="https://kit.fontawesome.com/f8ddf9854a.js" crossorigin="anonymous"></script>
    <meta charset="utf-8">
    <meta name="description"
          content="Towards The Future of Universal Omni-Language Models">
    <meta name="keywords" content="OmniBench, MLLM, MLLM Evaluation, Vision Language Model, Large Language Model, Large Multimodal Model, artificial intelligence, AI">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title> OmniBench: Towards The Future of Universal Omni-Language Models</title>

    <link rel="icon" href="./static/images/logo.jpg">

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
  </head>
  <body>

    <nav class="navbar" role="navigation" aria-label="main navigation">
      <div class="navbar-brand">
        <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
        </a>
      </div>
      <div class="navbar-menu">
        <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
          <div class="navbar-item has-dropdown is-hoverable">
            <a class="navbar-link">
              More Research
            </a>
            <div class="navbar-dropdown">
              <a class="navbar-item" href="https://github.com/multimodal-art-projection/MAP-NEO">
                MAP-NEO
              </a>
              <a class="navbar-item" href="https://github.com/yizhilll/MERT">
                MERT
              </a>
            </div>
          </div>
        </div>
      </div>
    </nav>

    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title is-bold">
                <img src="static/images/logo.jpg" style="width:1em;vertical-align: middle" alt="Logo"/>
                <span class="omnibench" style="vertical-align: middle">OmniBench</span>
              </h1>
              <h2 class="subtitle is-3 publication-subtitle">
                Towards The Future of Universal Omni-Language Models
              </h2>
              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a href="https://yizhilll.github.io/" style="text-decoration: none; color: inherit;">Yizhi Li*<sup style="color:#6fbf73;">1,2</sup></a>,
                </span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=qyTrq4kAAAAJ&hl" style="text-decoration: none; color: inherit;">Ge Zhang*<sup style="color:#6fbf73;">â€ 1,3</sup></a>,
                </span>
                <span class="author-block">
                  <a href="https://nicolaus625.github.io/" style="text-decoration: none; color: inherit;">Yinghao Ma*<sup style="color:#6fbf73;">1,4</sup></a>,
                </span>
                <span class="author-block">Ruibin Yuan<sup style="color:#6fbf73;">1,5</sup>,</span>
                <span class="author-block">Kang Zhu<sup style="color:#6fbf73;">1,3</sup>,</span><br>
                <span class="author-block">Hangyu Guo<sup style="color:#6fbf73;">1</sup>,</span>
                <span class="author-block">Yiming Liang<sup style="color:#6fbf73;">1</sup>,</span>
                <span class="author-block">Jiaheng Liu<sup style="color:#6fbf73;">1</sup>,</span>
                <span class="author-block">Jian Yang<sup style="color:#6fbf73;">1</sup>,</span>
                <span class="author-block">Siwei Wu<sup style="color:#6fbf73;">1,2</sup>,</span><br>
                <span class="author-block">Xingwei Qu<sup style="color:#6fbf73;">1,2</sup>,</span>
                <span class="author-block">Jinjie Shi<sup style="color:#6fbf73;">4</sup>,</span>
                <span class="author-block">Xinyue Zhang<sup style="color:#6fbf73;">1</sup>,</span>
                <span class="author-block">Zhenzhu Yang<sup style="color:#6fbf73;">1</sup>,</span>
                <span class="author-block">Xiangzhou Wang<sup style="color:#6fbf73;">1</sup>,</span><br>
                <span class="author-block">Zhaoxiang Zhang<sup style="color:#ed4b82;">6</sup>,</span>
                <span class="author-block">Zachary Liu<sup style="color:#9b51e0;">7</sup>,</span>
                <span class="author-block">
                  <a href="https://www.eecs.qmul.ac.uk/~emmanouilb/" style="text-decoration: none; color: inherit;">Emmanouil Benetos<sup style="color:#007bff;">4</sup></a>,
                </span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=OdE3MsQAAAAJ&hl" style="text-decoration: none; color: inherit;">Wenhao Huang<sup style="color:#6fbf73;">1,3</sup></a>,
                </span>
                <span class="author-block">
                  <a href="https://chenghualin.wordpress.com/" style="text-decoration: none; color: inherit;">Chenghua Lin<sup style="color:#b433ff;">â€ ,1,2</sup></a>,
                </span>

              </div>

              <br>

              <div class="is-size-5 publication-authors">
                <span class="author-block"><sup style="color:#6fbf73;">1</sup>m-a-p.ai,</span>
                <span class="author-block"><sup style="color:#b433ff;">2</sup>University of Manchester,</span>
                <span class="author-block"><sup style="color:#ed4b82;">3</sup>01.ai,</span>
                <span class="author-block"><sup style="color:#007bff;">4</sup>Queen Mary University of London,</span><br>
                <span class="author-block"><sup style="color:#ffac33;">5</sup>Hongkong University of Science and Technology,</span>
                <span class="author-block"><sup style="color:#ed4b82;">6</sup>Nanjing University,</span>
                <span class="author-block"><sup style="color:#9b51e0;">7</sup>Dartmouth College</span>
              </div>

              <br>
              <div class="is-size-5 publication-authors">
                <span class="author-block">*Core Contributors</span><br>
                <span class="author-block">â€ Corresponding to:</span>
                <span class="author-block"><a href="mailto:yizhi.li@hotmail.com">yizhi.li@hotmail.com</a>,</span>
                <span class="author-block"><a href="mailto:gezhang@umich.edu">gezhang@umich.edu</a>,</span>
                <span class="author-block"><a href="mailto:c.lin@manchester.ac.uk">c.lin@manchester.ac.uk</a></span>
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2311.16502" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://huggingface.co/datasets/m-a-p/OmniBench" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon" style="font-size:18px">ðŸ¤—</span>
                      <span>OminiBench</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://github.com/multimodal-art-projection/OmniBench" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>
                  <!-- <span class="link-block">
                    <a href="#leaderboard" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon has-text-white">
                        <i class="fa-solid fa-trophy"></i>
                      </span>
                      <span>Leaderboard</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://eval.ai/web/challenges/challenge-page/2179/overview" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon has-text-white">
                        <i class="fa-solid fa-medal"></i>
                      </span>
                      <span>EvalAI</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://twitter.com/xiangyue96/status/1729698316554801358" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon has-text-white">
                        <i class="fa-brands fa-x-twitter"></i>
                      </span>
                      <span>Twitter</span>
                    </a>
                  </span> -->
                  <span class="link-block">
                    <a href="https://m-a-p.ai/OmniBench/#examples" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon has-text-white">
                        <i class="fa-solid fa-book"></i>
                      </span>
                      <span>Examples</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <section class="hero teaser">
      <div class="container is-max-desktop has-text-centered">
        <!-- <img src="static/images/" alt="geometric reasoning"> -->
        <img src="static/images/category_example.jpg" alt="category_example" class="center" style="width: 150%; height: auto;">
        <!-- YIZHI --> 
        <!-- <p>Overview of the MMMU dataset. MMMU presents four challenges:
          1) <b>comprehensiveness</b>: 11.5K college-level problems across six broad disciplines and 30 college subjects;
          2) highly <b>heterogeneous</b> image types;
          3) <b>interleaved</b> text and images;
          4) <b>expert-level</b> perception and reasoning rooted in deep subject knowledge.
        </p> -->
      </div>
    </section>

    <style>
      .container {
          max-width: 800px !important; 
          margin: 0 auto;
      }
      .columns {
          margin: -0.5rem; 
      }
      .column {
          padding: 0.5rem;
      }
      .column.is-half {
          width: 50%;
          flex: none;
      }
      .card {
          display: flex;
          flex-direction: column;
          height: 100%;
      }
      .image-container {
          width: 100%;
          aspect-ratio: 4 / 3; 
          overflow: hidden;
          display: flex;
          justify-content: center;
          align-items: center;
          background-color: #f5f5f5;
      }
      .image-container img {
          max-width: 100%;
          max-height: 100%;
          object-fit: contain;
      }
      .card-content {
          padding: 0.5rem;
          display: flex;
          flex-direction: column;
          flex-grow: 1;
      }
      .audio-item .title.is-4 {
          font-size: 0.9rem;
          margin-bottom: 0.3rem;
      }
      audio {
          width: 100%;
          height: 30px;
          margin-top: auto;
      }
  </style>
    <section class="hero">
        <div class="hero-body">
            <div class="container" style="margin-bottom: 1vh;">
                <div class="columns is-centered has-text-centered">
                    <h1 class="subtitle is-4 publication-subtitle">Sample Audios with Images</h1>
                </div>
            </div>
        </div>
    </section>

    <section class="section" style="padding-top: 1rem; padding-bottom: 1rem;">
        <div class="container">
            <div class="columns is-multiline">
                <div class="column is-half">
                    <div class="card audio-item">
                        <div class="card-content">
                            <div class="image-container">
                                <img src="static/images/examples/action_and_activity.jpg" alt="Action and Activity">
                            </div>
                            <h3 class="title is-4">Action and Activity</h3>
                            <audio controls>
                                <source src="static/images/sample_audios/action_and_activity.mp3" type="audio/mpeg">
                                Your browser does not support the audio element.
                            </audio>
                        </div>
                    </div>
                </div>
                <div class="column is-half">
                    <div class="card audio-item">
                        <div class="card-content">
                            <div class="image-container">
                                <img src="static/images/examples/contextual_and_environmental.jpg" alt="Contextual and Environmental">
                            </div>
                            <h3 class="title is-4">Contextual and Environmental</h3>
                            <audio controls>
                                <source src="static/images/sample_audios/contextual_and_environmental.mp3" type="audio/mpeg">
                                Your browser does not support the audio element.
                            </audio>
                        </div>
                    </div>
                </div>
                <div class="column is-half">
                    <div class="card audio-item">
                        <div class="card-content">
                            <div class="image-container">
                                <img src="static/images/examples/count_and_quantity.jpg" alt="Count and Quantity">
                            </div>
                            <h3 class="title is-4">Count and Quantity</h3>
                            <audio controls>
                                <source src="static/images/sample_audios/count_and_quantity.mp3" type="audio/mpeg">
                                Your browser does not support the audio element.
                            </audio>
                        </div>
                    </div>
                </div>
                <div class="column is-half">
                    <div class="card audio-item">
                        <div class="card-content">
                            <div class="image-container">
                                <img src="static/images/examples/identity_and_relationship.jpg" alt="Identity and Relationship">
                            </div>
                            <h3 class="title is-4">Identity and Relationship</h3>
                            <audio controls>
                                <source src="static/images/sample_audios/identity_and_relationship.mp3" type="audio/mpeg">
                                Your browser does not support the audio element.
                            </audio>
                        </div>
                    </div>
                </div>
                <div class="column is-half">
                    <div class="card audio-item">
                        <div class="card-content">
                            <div class="image-container">
                                <img src="static/images/examples/object_identification_and_description.jpg" alt="Object Identification and Description">
                            </div>
                            <h3 class="title is-4">Object Identification and Description</h3>
                            <audio controls>
                                <source src="static/images/sample_audios/object_identification_and_description.mp3" type="audio/mpeg">
                                Your browser does not support the audio element.
                            </audio>
                        </div>
                    </div>
                </div>
                <div class="column is-half">
                    <div class="card audio-item">
                        <div class="card-content">
                            <div class="image-container">
                                <img src="static/images/examples/plot_inference.jpg" alt="Plot Inference">
                            </div>
                            <h3 class="title is-4">Plot Inference</h3>
                            <audio controls>
                                <source src="static/images/sample_audios/plot_inference.mp3" type="audio/mpeg">
                                Your browser does not support the audio element.
                            </audio>
                        </div>
                    </div>
                </div>
                <div class="column is-half">
                    <div class="card audio-item">
                        <div class="card-content">
                            <div class="image-container">
                                <img src="static/images/examples/story_description.jpg" alt="Story Description">
                            </div>
                            <h3 class="title is-4">Story Description</h3>
                            <audio controls>
                                <source src="static/images/sample_audios/story_description.mp3" type="audio/mpeg">
                                Your browser does not support the audio element.
                            </audio>
                        </div>
                    </div>
                </div>
                <div class="column is-half">
                    <div class="card audio-item">
                        <div class="card-content">
                            <div class="image-container">
                                <img src="static/images/examples/text_and_symbols.jpg" alt="Text and Symbols">
                            </div>
                            <h3 class="title is-4">Text and Symbols</h3>
                            <audio controls>
                                <source src="static/images/sample_audios/text_and_symbols.mp3" type="audio/mpeg">
                                Your browser does not support the audio element.
                            </audio>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <section class="section">
      <div class="container" style="margin-bottom: 2vh;">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">ðŸ””News</h2>
            <div class="content has-text-justified">
              <p>
                <b>ðŸ”¥[2024-09-22]: We release the new benchmark for text, image, and audio large language models!</b>
              </p>
          </div>
            <h2 class="title is-3">Introduction</h2>
            <div class="content has-text-justified">
              <p>
                Recent advancements in multimodal large language models (MLLMs) have aimed to integrate and interpret data across diverse modalities. However, the capacity of these models to concurrently process and reason about multiple modalities remains inadequately explored, partly due to the lack of comprehensive modality-wise benchmarks. We introduce OmniBench, a novel benchmark designed to rigorously evaluate models' ability to recognize, interpret, and reason across visual, acoustic, and textual inputs simultaneously. We define models capable of such tri-modal processing as omni-language models (OLMs).OmniBench is distinguished by high-quality human annotations, ensuring that accurate responses require integrated understanding and reasoning across all three modalities. Our main findings reveal that: i) open-source OLMs exhibit critical limitations in instruction-following and reasoning capabilities within tri-modal contexts; and ii) these baseline models perform poorly (below 50\% accuracy) even when provided with alternative textual representations of images and audio. These results suggest that the ability to construct a consistent context from text, image, and audio is often overlooked in existing MLLM training paradigms. We advocate for future research to focus on developing more robust tri-modal integration techniques and training strategies to enhance OLM performance across diverse modalities.
              </p>
            </div>
          </div>
        </div>
        <!--/ Abstract. -->
    </div>
    </section>

    <!-- DATASET SECTION -->
    <section class="hero is-light is-small">
      <div class="hero-body has-text-centered">
        <h1 class="title is-1 mmmu">
          <img src="static/images/logo.jpg" alt="Logo" class="mmmu-logo"/>
          <span class="mmmu">MMMU Benchmark</span>
        </h1>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Overview</h2>
            <div class="content has-text-justified">
              <p>
                We introduce the Massive Multi-discipline Multimodal Understanding and Reasoning (MMMU) benchmark, a novel benchmark meticulously curated to assess the expert-level multimodal understanding capability of foundation models across a broad scope of tasks. Covering subjects across disciplines, including Art, Business, Health & Medicine, Science, Humanities & Social Science, and Tech & Engineering, and over subfields. The detailed subject coverage and statistics are detailed in the figure. The questions in our benchmark were manually collected by a team of college students (including coauthors) from various disciplines and subjects, drawing from online sources, textbooks, and lecture materials.
              </p>
              <!-- <img src="static/images/category_example.jpg" alt="category_example" class="center"> -->
              <img src="static/images/annotation_scheme.jpg" alt="annotations" class="center" style="width: 150%; height: auto;">
              <br>
              <p>
        <!-- YIZHI --> 
                 <!-- MMMU is designed to measure three essential skills in LMMs: perception, knowledge, and reasoning. Our aim is to evaluate how well these models can not only perceive and understand information across different modalities but also apply reasoning with subject-specific knowledge to derive the solution. -->
              </p>
              <p>
        <!-- YIZHI --> 
                <!-- Our MMMU benchmark introduces key challenges to multimodal foundation models, as detailed in a figure. Among these, we particularly highlight the challenge stemming from the requirement for both expert-level visual perceptual abilities and deliberate reasoning with subject-specific knowledge. This challenge is vividly illustrated through our tasks, which not only demand the processing of various heterogeneous image types but also necessitate a model's adeptness in using domain-specific knowledge to deeply understand both the text and images and to reason. This goes significantly beyond basic visual perception, calling for an advanced approach that integrates advanced multimodal analysis with domain-specific knowledge. -->
              </p>
            </div>
          </div>
        </div>

        <!-- <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Comparisons with Existing Benchmarks</h2>
            <div class="content has-text-justified">
              <p>
                To further distinguish the difference between <i>dataset</i> and other existing ones, we elaborate the benchmark details in Figure.
                From the <i>breadth</i> perspective, the prior benchmarks are heavily focused on daily knowledge and common sense.
                The covered image format is also limited. Our benchmark aims to cover college-level knowledge with 30 image formats including diagrams,
                tables, charts, chemical structures, photos, paintings, geometric shapes, music sheets, medical images, etc.
                In the <i>depth</i> aspect, the previous benchmarks normally require commonsense knowledge or simple physical or temporal reasoning.
                In contrast, our benchmark requires deliberate reasoning with college-level subject knowledge.
              </p>
              <div class="content has-text-centered">
                <img src="static/images/compare.Jpeg" alt="algebraic reasoning" class="center">
                <p> Sampled MMMU examples from each discipline. The questions and images need expert-level knowledge to understand and reason.</p>
              </div>
            </div>
          </div>
        </div> -->

        <div class="columns is-centered m-6">
          <div class="column is-full has-text-centered content">
            <h2 class="title is-3">Statistics</h2>
            <div class="carousel results-carousel">
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="static/images/inspection_distribution.jpg" alt="algebraic reasoning" width="95%"/>
                  <p> Sampled MMMU examples from each discipline. The questions and images need expert-level knowledge to understand and reason.</p>
                </div>
              </div>
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="static/images/data_distribution.jpg" alt="arithmetic reasoning" width="120%"/>
                  <p> Key statistics of the MMMU benchmark</p>
                </div>
              </div>
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="static/images/table.jpg" alt="arithmetic reasoning" width="100%"/>
                  <p> Distribution of image types in the MMMU dataset</p>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- RESULTS SECTION -->
    <section class="hero is-light is-small">
      <div class="hero-body has-text-centered">
        <h1 class="title is-1 mmmu">Experiment Results</h1>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <!-------------------------------------------------------------------- RESULTS SECTION -------------------------------------------------------------------->
        <div class="columns is-centered m-6">
          <div class="column is-full has-text-centered content">
            <h2 class="title is-3" id="leaderboard">Leaderboard</h2>
            <div class="content has-text-justified">
              <p>
        <!-- YIZHI --> 
                <!-- We evaluate various models including LLMs and LMMs.
                In each type, we consider both closed- and open-source models.
                Our evaluation is conducted under a zero-shot setting to assess the capability of models to generate accurate answers without fine-tuning or few-shot demonstrations on our benchmark.
                For all models, we use the default prompt provided by each model for multi-choice or open QA, if available.
                If models do not provide prompts for task types in MMMU, we conduct prompt engineering on the validation set and use the most effective prompt for the later zero-shot experiment. -->
              </p>
            </div>
            <br>
            <div class="model-labels-container">
              <span class="leaderboard-label human_expert">Human Expert</span>
              <span class="leaderboard-label open_source">Open-Source</span>
              <span class="leaderboard-label proprietary">Proprietary</span>
            </div>
            <br>
            <div class="content has-text-centered">
              <p>
        <!-- YIZHI --> 
                <!-- Click on MMMU-Pro, MMMU (Val) or MMMU (Test) to expand detailed results. -->
              </p>
            </div>
        <!-------------------------------------------------------------------- Difficulty Levels SECTION -------------------------------------------------------------------->
    </section>

    <!-- @PAN TODO: bibtex -->
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title is-3 has-text-centered">BibTeX</h2>
        <pre><code>
        <!-- YIZHI --> 
    </code></pre>
      </div>
    </section>

    <footer class="footer">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is website adapted from <a href="https://nerfies.github.io/">Nerfies</a> and <a href="https://mathvista.github.io/">MathVista</a>, licensed under a <a rel="license"
                                                  href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
          </div>
        </div>
      </div>
    </footer>

  </body>
</html>
