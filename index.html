<!DOCTYPE html>
<html lang="en">
  <head>
    <title>OmniBench</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="https://kit.fontawesome.com/f8ddf9854a.js" crossorigin="anonymous"></script>
    <meta charset="utf-8">
    <meta name="description"
          content="Towards The Future of Universal Omni-Language Models">
    <meta name="keywords" content="OmniBench, MLLM, MLLM Evaluation, Vision Language Model, Large Language Model, Large Multimodal Model, artificial intelligence, AI">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title> OmniBench: Towards The Future of Universal Omni-Language Models</title>

    <link rel="icon" href="./static/images/logo.jpg">

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
  </head>
  <body>

    <nav class="navbar" role="navigation" aria-label="main navigation">
      <div class="navbar-brand">
        <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
        </a>
      </div>
      <div class="navbar-menu">
        <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
          <div class="navbar-item has-dropdown is-hoverable">
            <a class="navbar-link">
              More Research
            </a>
            <div class="navbar-dropdown">
              <a class="navbar-item" href="https://github.com/multimodal-art-projection/MAP-NEO">
                MAP-NEO
              </a>
              <a class="navbar-item" href="https://github.com/yizhilll/MERT">
                MERT
              </a>
            </div>
          </div>
        </div>
      </div>
    </nav>

    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title is-bold">
                <img src="static/images/logo.jpg" style="width:1em;vertical-align: middle" alt="Logo"/>
                <span class="omnibench" style="vertical-align: middle">OmniBench</span>
              </h1>
              <h2 class="subtitle is-3 publication-subtitle">
                Towards The Future of Universal Omni-Language Models
              </h2>
              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a href="https://yizhilll.github.io/" style="text-decoration: none; color: inherit;">Yizhi Li*<sup style="color:#6fbf73;">1,2</sup></a>,
                </span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=qyTrq4kAAAAJ&hl" style="text-decoration: none; color: inherit;">Ge Zhang*<sup style="color:#6fbf73;">â€ 1,3</sup></a>,
                </span>
                <span class="author-block">
                  <a href="https://nicolaus625.github.io/" style="text-decoration: none; color: inherit;">Yinghao Ma*<sup style="color:#6fbf73;">1,4</sup></a>,
                </span>
                <span class="author-block">Ruibin Yuan<sup style="color:#6fbf73;">1,5</sup>,</span>
                <span class="author-block">Kang Zhu<sup style="color:#6fbf73;">1,3</sup>,</span><br>
                <span class="author-block">Hangyu Guo<sup style="color:#6fbf73;">1</sup>,</span>
                <span class="author-block">Yiming Liang<sup style="color:#6fbf73;">1</sup>,</span>
                <span class="author-block">Jiaheng Liu<sup style="color:#6fbf73;">1</sup>,</span>
                <span class="author-block">Jian Yang<sup style="color:#6fbf73;">1</sup>,</span>
                <span class="author-block">Siwei Wu<sup style="color:#6fbf73;">1,2</sup>,</span><br>
                <span class="author-block">Xingwei Qu<sup style="color:#6fbf73;">1,2</sup>,</span>
                <span class="author-block">Jinjie Shi<sup style="color:#6fbf73;">4</sup>,</span>
                <span class="author-block">Xinyue Zhang<sup style="color:#6fbf73;">1</sup>,</span>
                <span class="author-block">Zhenzhu Yang<sup style="color:#6fbf73;">1</sup>,</span>
                <span class="author-block">Xiangzhou Wang<sup style="color:#6fbf73;">1</sup>,</span><br>
                <span class="author-block">Zhaoxiang Zhang<sup style="color:#ed4b82;">6</sup>,</span>
                <span class="author-block">Zachary Liu<sup style="color:#9b51e0;">7</sup>,</span>
                <span class="author-block">
                  <a href="https://www.eecs.qmul.ac.uk/~emmanouilb/" style="text-decoration: none; color: inherit;">Emmanouil Benetos<sup style="color:#007bff;">4</sup></a>,
                </span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=OdE3MsQAAAAJ&hl" style="text-decoration: none; color: inherit;">Wenhao Huang<sup style="color:#6fbf73;">1,3</sup></a>,
                </span>
                <span class="author-block">
                  <a href="https://chenghualin.wordpress.com/" style="text-decoration: none; color: inherit;">Chenghua Lin<sup style="color:#b433ff;">â€ ,1,2</sup></a>,
                </span>

              </div>

              <br>

              <div class="is-size-5 publication-authors">
                <span class="author-block"><sup style="color:#6fbf73;">1</sup>m-a-p.ai,</span>
                <span class="author-block"><sup style="color:#b433ff;">2</sup>University of Manchester,</span>
                <span class="author-block"><sup style="color:#ed4b82;">3</sup>01.ai,</span>
                <span class="author-block"><sup style="color:#007bff;">4</sup>Queen Mary University of London,</span><br>
                <span class="author-block"><sup style="color:#ffac33;">5</sup>Hongkong University of Science and Technology,</span>
                <span class="author-block"><sup style="color:#ed4b82;">6</sup>Nanjing University,</span>
                <span class="author-block"><sup style="color:#9b51e0;">7</sup>Dartmouth College</span>
              </div>

              <br>
              <div class="is-size-5 publication-authors">
                <span class="author-block">*Core Contributors</span><br>
                <span class="author-block">â€ Corresponding to:</span>
                <span class="author-block"><a href="mailto:yizhi.li@hotmail.com">yizhi.li@hotmail.com</a>,</span>
                <span class="author-block"><a href="mailto:gezhang@umich.edu">gezhang@umich.edu</a>,</span>
                <span class="author-block"><a href="mailto:c.lin@manchester.ac.uk">c.lin@manchester.ac.uk</a></span>
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2311.16502" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://huggingface.co/datasets/m-a-p/OmniBench" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon" style="font-size:18px">ðŸ¤—</span>
                      <span>OminiBench</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://github.com/multimodal-art-projection/OmniBench" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>
                  <!-- <span class="link-block">
                    <a href="#leaderboard" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon has-text-white">
                        <i class="fa-solid fa-trophy"></i>
                      </span>
                      <span>Leaderboard</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://eval.ai/web/challenges/challenge-page/2179/overview" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon has-text-white">
                        <i class="fa-solid fa-medal"></i>
                      </span>
                      <span>EvalAI</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://twitter.com/xiangyue96/status/1729698316554801358" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon has-text-white">
                        <i class="fa-brands fa-x-twitter"></i>
                      </span>
                      <span>Twitter</span>
                    </a>
                  </span> -->
                  <span class="link-block">
                    <a href="https://m-a-p.ai/OmniBench/#examples" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon has-text-white">
                        <i class="fa-solid fa-book"></i>
                      </span>
                      <span>Examples</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <section class="section">
      <div class="container" style="margin-bottom: 2vh;">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">ðŸ””News</h2>
            <div class="content has-text-justified">
              <p>
                <b>ðŸ”¥[2024-09-22]: We release the new benchmark for text, image, and audio large language models!</b>
              </p>
          </div>
            <h2 class="title is-3">Introduction</h2>
            <div class="content has-text-justified">
              <p>
                Recent advancements in multimodal large language models (MLLMs) have aimed to integrate and interpret data across diverse modalities. However, the capacity of these models to concurrently process and reason about multiple modalities remains inadequately explored, partly due to the lack of comprehensive modality-wise benchmarks. We introduce OmniBench, a novel benchmark designed to rigorously evaluate models' ability to recognize, interpret, and reason across visual, acoustic, and textual inputs simultaneously. We define models capable of such tri-modal processing as omniData -languOmniBench is distinguished by high-quality human annotations, ensuring that accurate responses require integrated understanding and reasoning across all three modalities. Our main findings reveal that: i) open-source OLMs exhibit critical limitations in instruction-following and reasoning capabilities within tri-modal contexts; and ii) these baseline models perform poorly (below 50% accuracy) even when provided with alternative textual representations of images and audio. These results suggest that the ability to construct a consistent context from text, image, and audio is often overlooked in existing MLLM training paradigms. We advocate for future research to focus on developing more robust tri-modal integration techniques and training strategies to enhance OLM performance across diverse modalities.
              </p>
            </div>
          </div>
        </div>
        <!--/ Abstract. -->
    </div>
    </section>

    <style>
      .container {
          max-width: 800px !important; 
          margin: 0 auto;
      }
      .columns {
          margin: -0.5rem; 
      }
      .column {
          padding: 0.5rem;
      }
      .column.is-half {
          width: 50%;
          flex: none;
      }
      .card {
          display: flex;
          flex-direction: column;
          height: 100%;
      }
      .image-container {
          width: 100%;
          aspect-ratio: 4 / 3; 
          overflow: hidden;
          display: flex;
          justify-content: center;
          align-items: center;
          background-color: #f5f5f5;
      }
      .image-container img {
          max-width: 100%;
          max-height: 100%;
          object-fit: contain;
      }
      .card-content {
          padding: 0.5rem;
          display: flex;
          flex-direction: column;
          flex-grow: 1;
      }
      .audio-item .title.is-4 {
          font-size: 0.9rem;
          margin-bottom: 0.3rem;
      }
      audio {
          width: 100%;
          height: 30px;
          margin-top: auto;
      }
  </style>
    <section class="hero">
        <div class="hero-body">
            <div class="container" style="margin-bottom: 1vh;">
                <div class="columns is-centered has-text-centered">
                    <h1 class="subtitle is-4 publication-subtitle">Data Samples Across Categories</h1>
                </div>
            </div>
        </div>
    </section>

    <section class="section" style="padding-top: 1rem; padding-bottom: 1rem;">
        <div class="container">
            <div class="columns is-multiline">
                <div class="column is-half">
                    <div class="card audio-item">
                        <div class="card-content">
                            <div class="image-container">
                                <img src="static/images/examples/action_and_activity.jpg" alt="Action and Activity">
                            </div>
                            <h3 class="title is-4">Action and Activity</h3>
                            <audio controls>
                                <source src="static/images/sample_audios/action_and_activity.mp3" type="audio/mpeg">
                                Your browser does not support the audio element.
                            </audio>
                        </div>
                    </div>
                </div>
                <div class="column is-half">
                    <div class="card audio-item">
                        <div class="card-content">
                            <div class="image-container">
                                <img src="static/images/examples/contextual_and_environmental.jpg" alt="Contextual and Environmental">
                            </div>
                            <h3 class="title is-4">Contextual and Environmental</h3>
                            <audio controls>
                                <source src="static/images/sample_audios/contextual_and_environmental.mp3" type="audio/mpeg">
                                Your browser does not support the audio element.
                            </audio>
                        </div>
                    </div>
                </div>
                <div class="column is-half">
                    <div class="card audio-item">
                        <div class="card-content">
                            <div class="image-container">
                                <img src="static/images/examples/count_and_quantity.jpg" alt="Count and Quantity">
                            </div>
                            <h3 class="title is-4">Count and Quantity</h3>
                            <audio controls>
                                <source src="static/images/sample_audios/count_and_quantity.mp3" type="audio/mpeg">
                                Your browser does not support the audio element.
                            </audio>
                        </div>
                    </div>
                </div>
                <div class="column is-half">
                    <div class="card audio-item">
                        <div class="card-content">
                            <div class="image-container">
                                <img src="static/images/examples/identity_and_relationship.jpg" alt="Identity and Relationship">
                            </div>
                            <h3 class="title is-4">Identity and Relationship</h3>
                            <audio controls>
                                <source src="static/images/sample_audios/identity_and_relationship.mp3" type="audio/mpeg">
                                Your browser does not support the audio element.
                            </audio>
                        </div>
                    </div>
                </div>
                <div class="column is-half">
                    <div class="card audio-item">
                        <div class="card-content">
                            <div class="image-container">
                                <img src="static/images/examples/object_identification_and_description.jpg" alt="Object Identification and Description">
                            </div>
                            <h3 class="title is-4">Object Identification and Description</h3>
                            <audio controls>
                                <source src="static/images/sample_audios/object_identification_and_description.mp3" type="audio/mpeg">
                                Your browser does not support the audio element.
                            </audio>
                        </div>
                    </div>
                </div>
                <div class="column is-half">
                    <div class="card audio-item">
                        <div class="card-content">
                            <div class="image-container">
                                <img src="static/images/examples/plot_inference.jpg" alt="Plot Inference">
                            </div>
                            <h3 class="title is-4">Plot Inference</h3>
                            <audio controls>
                                <source src="static/images/sample_audios/plot_inference.mp3" type="audio/mpeg">
                                Your browser does not support the audio element.
                            </audio>
                        </div>
                    </div>
                </div>
                <div class="column is-half">
                    <div class="card audio-item">
                        <div class="card-content">
                            <div class="image-container">
                                <img src="static/images/examples/story_description.jpg" alt="Story Description">
                            </div>
                            <h3 class="title is-4">Story Description</h3>
                            <audio controls>
                                <source src="static/images/sample_audios/story_description.mp3" type="audio/mpeg">
                                Your browser does not support the audio element.
                            </audio>
                        </div>
                    </div>
                </div>
                <div class="column is-half">
                    <div class="card audio-item">
                        <div class="card-content">
                            <div class="image-container">
                                <img src="static/images/examples/text_and_symbols.jpg" alt="Text and Symbols">
                            </div>
                            <h3 class="title is-4">Text and Symbols</h3>
                            <audio controls>
                                <source src="static/images/sample_audios/text_and_symbols.mp3" type="audio/mpeg">
                                Your browser does not support the audio element.
                            </audio>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- DATASET SECTION -->
    <section class="hero is-light is-small">
      <div class="hero-body has-text-centered">
        <h1 class="title is-1 mmmu">
          <img src="static/images/logo.jpg" alt="Logo" class="mmmu-logo"/>
          <span class="mmmu">OmniBench</span>
        </h1>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Overview</h2>
            <div class="content has-text-justified">
              <p>
                The OmniBench aims to create the first comprehensive benchmark for evaluating multimodal large language models that support simultaneous image, audio, and text inputs. While OmniBench is designed to evaluate the understanding capability of MLLMs on cross-modality complementary information, the models are required to interpret the multimodal input and provide accurate text answer.
                The problem could be formulated as following: given a tuple of (image, audio, text), the model is requito recognize the objects, re-build the contexts, and conduct reasoning based on the given information.
                The design logic and statistics of the dataset and the annotation protocols are introduced in this section.
              </p>
              <!-- <img src="static/images/category_example.jpg" alt="category_example" class="center"> -->
              <img src="static/images/table.jpg" alt="data_stats" class="center" style="width: 100%; height: auto;">
              <br>
              <p>
                We propose a novel task type categorization in OmniBench that assesses a broad spectrum of reasoning and cognitive abilities. Our taxonomy progresses from fundamental perception (Object Identification & Description) to complex inference (Contextual & Environmental, Identity & Relationship). It incorporates temporal and logical order understanding of events (Action & Activity, Story Description, Plot Inference), spatial awareness (Contextual & Environmental), entity recognition (Object Identification & Description), symbolic processing (Text & Symbols), and quantitative reasoning (Count & Quantity). This comprehensive design evaluates both low-level perceptual skills and high-level cognitive functions, enabling a holistic assessment of multimodal language models' (MLLMs) capabilities to recognize, describe, integrate information, understand context, and make nuanced inferences.
                OmniBench comprises 1142 question-answer pairs, with task type distribution, text length, and image and audio characteristics. The dataset's audio content falls into three categories: speech (human vocal communication), sound events (non-speech natural, environmental and mechanical sounds), and music (various compositions and performances). 
              </p>
              <br>
              <img src="static/images/annotation_scheme.jpg" alt="annotation" class="center" style="width: 100%; height: auto;"> 
              <br>
              <p>
                Our annotation scheme is built upon a fundamental principle: the correct answer to each question must require information from both the image and audio components. This ensures that the benchmark effectively evaluates the model's ability to analyze information across modalities. Our quality control process was two-fold, including human inspection round and automatic inspection round assisted by MLLM. 
              </p>
            </div>
          </div>
        </div>

        <!-- <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Comparisons with Existing Benchmarks</h2>
            <div class="content has-text-justified">
              <p>
                To further distinguish the difference between <i>dataset</i> and other existing ones, we elaborate the benchmark details in Figure.
                From the <i>breadth</i> perspective, the prior benchmarks are heavily focused on daily knowledge and common sense.
                The covered image format is also limited. Our benchmark aims to cover college-level knowledge with 30 image formats including diagrams,
                tables, charts, chemical structures, photos, paintings, geometric shapes, music sheets, medical images, etc.
                In the <i>depth</i> aspect, the previous benchmarks normally require commonsense knowledge or simple physical or temporal reasoning.
                In contrast, our benchmark requires deliberate reasoning with college-level subject knowledge.
              </p>
              <div class="content has-text-centered">
                <img src="static/images/compare.Jpeg" alt="algebraic reasoning" class="center">
                <p> Sampled MMMU examples from each discipline. The questions and images need expert-level knowledge to understand and reason.</p>
              </div>
            </div>
          </div>
        </div> -->

        <div class="columns is-centered m-6">
          <div class="column is-full has-text-centered content">
            <h2 class="title is-3">Statistics</h2>
            <div class="carousel results-carousel">
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="static/images/inspection_distribution.jpg" alt="algebraic reasoning" width="95%"/>
                  <p> The Distribution of Inspection Frequency of the Passed Samples in OmniBench.</p>
                </div>
              </div>
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="static/images/data_distribution.jpg" alt="arithmetic reasoning" width="120%"/>
                  <p> The Data Distribution of OmniBench. Each chart represents the data samples grouped by
                    task types and distinguished by audio types.</p>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- RESULTS SECTION -->
    <section class="hero is-light is-small">
      <div class="hero-body has-text-centered">
        <h1 class="title is-1 mmmu">Experiment Results</h1>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <!-------------------------------------------------------------------- RESULTS SECTION -------------------------------------------------------------------->
        <div class="columns is-centered m-6">
          <div class="column is-full has-text-centered content">
            <h2 class="title is-3" id="leaderboard">Leaderboard</h2>
            <div class="content has-text-justified">
              <p>
                The main focus of OmniBench is to evaluate how well could the omni-language models (OLMs) understand and reconstruct the context given information from image, audio and text modalities.
                Setting up questions with four available options for the models, we use accuracy, i.e., the ratio matched letter of the correct option and model response, as the evaluation metric (n.b., the accuracy of a random guess model is 25% under this setting).
              </p>
            </div>
            <br>
            <div class="model-labels-container">
              <!-- <span class="leaderboard-label human_expert">Human Expert</span> -->
              <span class="leaderboard-label open_source_olm">Open-Source OLM</span>
              <span class="leaderboard-label proprietary_olm">Proprietary OLM</span>
              <br><br>
              <span class="leaderboard-label open_source_vlmalm">Open-Source VLM or ALM</span>
              <span class="leaderboard-label proprietary_vlmalm">Proprietary VLM or ALM</span>
            </div>
            <br>
            <div class="content has-text-centered">
              <p>
                The first row suggets the input context, where "Img. & Aud." refers to vanilla image and audio, and "(T)" refers to the textual alternative of image and audio.
                Click on the 4 setting columns to expand detailed results.
              </p>
            </div>
            <div class="leaderboard-container">
              <div class="table-wrapper">
                <table id="mmmu-table">
                  <thead>
                    <tr>
                      <th colspan="3" class="reset-cell clickable" style="text-align: center;">Reset</th>
                      <th class="IA-details-cell clickable" colspan="1">Img. & Aud.</th>
                      <th class="TI-details-cell clickable" colspan="1">Img.(T) & Aud.</th>
                      <th class="TA-details-cell clickable" colspan="1">Img. & Aud. (T)</th>
                      <th class="TIA-details-cell clickable" colspan="1">Img. (T) & Aud. (T)</th>
                    </tr>
                    <tr>
                      <th class="sortable clickable" data-sort="string">Name</th>
                      <th class="clickable" data-sort="string">Size</th>
                      <th class="sortable clickable" data-sort="date">Date</th>
                        <th class="sortable clickable IA-overall" data-sort="number">Overall</th>
                        <th class="hidden IA-details sortable clickable" data-sort="number">Action & Activity</th>
                        <th class="hidden IA-details sortable clickable" data-sort="number">Story Description</th>
                        <th class="hidden IA-details sortable clickable" data-sort="number">Plot Inference</th>
                        <th class="hidden IA-details sortable clickable" data-sort="number">Object Identification & Description</th>
                        <th class="hidden IA-details sortable clickable" data-sort="number">Contextual & Environmental</th>
                        <th class="hidden IA-details sortable clickable" data-sort="number">Identity & Relationship</th>
                        <th class="hidden IA-details sortable clickable" data-sort="number">Text & Symbols</th>
                        <th class="hidden IA-details sortable clickable" data-sort="number">Count & Quantity</th>
                        <th class="sortable clickable TI-overall" data-sort="number">Overall</th>
                        <th class="hidden TI-details sortable clickable" data-sort="number">Action & Activity</th>
                        <th class="hidden TI-details sortable clickable" data-sort="number">Story Description</th>
                        <th class="hidden TI-details sortable clickable" data-sort="number">Plot Inference</th>
                        <th class="hidden TI-details sortable clickable" data-sort="number">Object Identification & Description</th>
                        <th class="hidden TI-details sortable clickable" data-sort="number">Contextual & Environmental</th>
                        <th class="hidden TI-details sortable clickable" data-sort="number">Identity & Relationship</th>
                        <th class="hidden TI-details sortable clickable" data-sort="number">Text & Symbols</th>
                        <th class="hidden TI-details sortable clickable" data-sort="number">Count & Quantity</th>
                        <th class="sortable clickable TA-overall" data-sort="number">Overall</th>
                        <th class="hidden TA-details sortable clickable" data-sort="number">Action & Activity</th>
                        <th class="hidden TA-details sortable clickable" data-sort="number">Story Description</th>
                        <th class="hidden TA-details sortable clickable" data-sort="number">Plot Inference</th>
                        <th class="hidden TA-details sortable clickable" data-sort="number">Object Identification & Description</th>
                        <th class="hidden TA-details sortable clickable" data-sort="number">Contextual & Environmental</th>
                        <th class="hidden TA-details sortable clickable" data-sort="number">Identity & Relationship</th>
                        <th class="hidden TA-details sortable clickable" data-sort="number">Text & Symbols</th>
                        <th class="hidden TA-details sortable clickable" data-sort="number">Count & Quantity</th>
                        <th class="sortable clickable TIA-overall" data-sort="number">Overall</th>
                        <th class="hidden TIA-details sortable clickable" data-sort="number">Action & Activity</th>
                        <th class="hidden TIA-details sortable clickable" data-sort="number">Story Description</th>
                        <th class="hidden TIA-details sortable clickable" data-sort="number">Plot Inference</th>
                        <th class="hidden TIA-details sortable clickable" data-sort="number">Object Identification & Description</th>
                        <th class="hidden TIA-details sortable clickable" data-sort="number">Contextual & Environmental</th>
                        <th class="hidden TIA-details sortable clickable" data-sort="number">Identity & Relationship</th>
                        <th class="hidden TIA-details sortable clickable" data-sort="number">Text & Symbols</th>
                        <th class="hidden TIA-details sortable clickable" data-sort="number">Count & Quantity</th>
                    </tr>
                  </thead>
                  <tbody>
                    <!-- Table body will be populated dynamically -->
                  </tbody>
                </table>
                <p class="test-desc"> Overall results of different models on the Omnibench leaderboard.</p>
              </div>
            </div>
            </div>
        <!-------------------------------------------------------------------- Difficulty Levels SECTION -------------------------------------------------------------------->
    </section>

    <!-- @PAN TODO: bibtex -->
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title is-3 has-text-centered">BibTeX</h2>
        <pre><code>
    @misc{li2024omnibench,
        title={OmniBench: Towards The Future of Universal Omni-Language Models}, 
        author={Yizhi Li and Ge Zhang and Yinghao Ma and Ruibin Yuan and Kang Zhu and Hangyu Guo and Yiming Liang and Jiaheng Liu and Jian Yang and Siwei Wu and Xingwei Qu and Jinjie Shi and Xinyue Zhang and Zhenzhu Yang and Xiangzhou Wang and Zhaoxiang Zhang and Zachary Liu and Emmanouil Benetos and Wenhao Huang and Chenghua Lin},
        year={2024},
        eprint={2409.15272},
        archivePrefix={arXiv},
        primaryClass={cs.CL},
        url={https://arxiv.org/abs/2409.15272}, 
    }
    </code></pre>
      </div>
    </section>

    <footer class="footer">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is website adapted from <a href="https://nerfies.github.io/">Nerfies</a> and <a href="https://mathvista.github.io/">MathVista</a>, licensed under a <a rel="license"
                                                  href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
          </div>
        </div>
      </div>
    </footer>

  </body>
</html>
