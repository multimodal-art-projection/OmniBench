<!DOCTYPE html>
<html lang="en">
  <head>
    <title>OmniBench</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="https://kit.fontawesome.com/f8ddf9854a.js" crossorigin="anonymous"></script>
    <meta charset="utf-8">
    <meta name="description"
          content="Towards The Future of Universal Omni-Language Models">
    <meta name="keywords" content="OmniBench, MLLM, MLLM Evaluation, Vision Language Model, Large Language Model, Large Multimodal Model, artificial intelligence, AI">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title> OmniBench: Towards The Future of Universal Omni-Language Models</title>

    <link rel="icon" href="./static/images/logo.png">

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
  </head>
  <body>

    <nav class="navbar" role="navigation" aria-label="main navigation">
      <div class="navbar-brand">
        <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
        </a>
      </div>
      <div class="navbar-menu">
        <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
          <div class="navbar-item has-dropdown is-hoverable">
            <a class="navbar-link">
              More Research
            </a>
            <div class="navbar-dropdown">
              <a class="navbar-item" href="https://github.com/multimodal-art-projection/MAP-NEO">
                MAP-NEO
              </a>
              <a class="navbar-item" href="https://github.com/yizhilll/MERT">
                MERT
              </a>
            </div>
          </div>
        </div>
      </div>
    </nav>

    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title is-bold">
                <img src="static/images/logo.png" style="width:1em;vertical-align: middle" alt="Logo"/>
                <span class="omnibench" style="vertical-align: middle">OmniBench</span>
              </h1>
              <h2 class="subtitle is-3 publication-subtitle">
                Towards The Future of Universal Omni-Language Models
              </h2>
              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a href="https://yizhilll.github.io/" style="text-decoration: none; color: inherit;">Yizhi Li*<sup style="color:#6fbf73;">1,2</sup></a>,
                </span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=qyTrq4kAAAAJ&hl" style="text-decoration: none; color: inherit;">Ge Zhang*<sup style="color:#6fbf73;">‚Ä†1,3</sup></a>,
                </span>
                <span class="author-block">
                  <a href="https://nicolaus625.github.io/" style="text-decoration: none; color: inherit;">Yinghao Ma*<sup style="color:#6fbf73;">1,4</sup></a>,
                </span>
                <span class="author-block">Ruibin Yuan<sup style="color:#6fbf73;">1,5</sup>,</span>
                <span class="author-block">Kang Zhu<sup style="color:#6fbf73;">1,3</sup>,</span><br>
                <span class="author-block">Hangyu Guo<sup style="color:#6fbf73;">1</sup>,</span>
                <span class="author-block">Yiming Liang<sup style="color:#6fbf73;">1</sup>,</span>
                <span class="author-block">Jiaheng Liu<sup style="color:#6fbf73;">1</sup>,</span>
                <span class="author-block">Jian Yang<sup style="color:#6fbf73;">1</sup>,</span>
                <span class="author-block">Siwei Wu<sup style="color:#6fbf73;">1,2</sup>,</span><br>
                <span class="author-block">Xingwei Qu<sup style="color:#6fbf73;">1,2</sup>,</span>
                <span class="author-block">Jinjie Shi<sup style="color:#6fbf73;">4</sup>,</span>
                <span class="author-block">Xinyue Zhang<sup style="color:#6fbf73;">1</sup>,</span>
                <span class="author-block">Zhenzhu Yang<sup style="color:#6fbf73;">1</sup>,</span>
                <span class="author-block">Xiangzhou Wang<sup style="color:#6fbf73;">1</sup>,</span><br>
                <span class="author-block">Zhaoxiang Zhang<sup style="color:#ed4b82;">6</sup>,</span>
                <span class="author-block">Zachary Liu<sup style="color:#9b51e0;">7</sup>,</span>
                <span class="author-block">
                  <a href="https://www.eecs.qmul.ac.uk/~emmanouilb/" style="text-decoration: none; color: inherit;">Emmanouil Benetos<sup style="color:#007bff;">4</sup></a>,
                </span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=OdE3MsQAAAAJ&hl" style="text-decoration: none; color: inherit;">Wenhao Huang<sup style="color:#6fbf73;">1,3</sup></a>,
                </span>
                <span class="author-block">
                  <a href="https://chenghualin.wordpress.com/" style="text-decoration: none; color: inherit;">Chenghua Lin<sup style="color:#b433ff;">‚Ä†,1,2</sup></a>,
                </span>

              </div>

              <br>

              <div class="is-size-5 publication-authors">
                <span class="author-block"><sup style="color:#6fbf73;">1</sup>m-a-p.ai,</span>
                <span class="author-block"><sup style="color:#b433ff;">2</sup>University of Manchester,</span>
                <span class="author-block"><sup style="color:#ed4b82;">3</sup>01.ai,</span>
                <span class="author-block"><sup style="color:#007bff;">4</sup>Queen Mary University of London,</span><br>
                <span class="author-block"><sup style="color:#ffac33;">5</sup>Hongkong University of Science and Technology,</span>
                <span class="author-block"><sup style="color:#ed4b82;">6</sup>Nanjing University,</span>
                <span class="author-block"><sup style="color:#9b51e0;">7</sup>Dartmouth College</span>
              </div>

              <br>
              <div class="is-size-5 publication-authors">
                <span class="author-block">*Core Contributors</span><br>
                <span class="author-block">‚Ä†Corresponding to:</span>
                <span class="author-block"><a href="mailto:yizhi.li@hotmail.com">yizhi.li@hotmail.com</a>,</span>
                <span class="author-block"><a href="mailto:gezhang@umich.edu">gezhang@umich.edu</a>,</span>
                <span class="author-block"><a href="mailto:c.lin@manchester.ac.uk">c.lin@manchester.ac.uk</a></span>
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2311.16502" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://huggingface.co/datasets/MMMU/MMMU_Pro" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon" style="font-size:18px">ü§ó</span>
                      <span>MMMU-Pro</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://huggingface.co/datasets/MMMU/MMMU" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon" style="font-size:18px">ü§ó</span>
                      <span>MMMU</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://github.com/MMMU-Benchmark/MMMU" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="#leaderboard" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon has-text-white">
                        <i class="fa-solid fa-trophy"></i>
                      </span>
                      <span>Leaderboard</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://eval.ai/web/challenges/challenge-page/2179/overview" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon has-text-white">
                        <i class="fa-solid fa-medal"></i>
                      </span>
                      <span>EvalAI</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://twitter.com/xiangyue96/status/1729698316554801358" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon has-text-white">
                        <i class="fa-brands fa-x-twitter"></i>
                      </span>
                      <span>Twitter</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="#examples" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon has-text-white">
                        <i class="fa-solid fa-book"></i>
                      </span>
                      <span>Examples</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="hero teaser">
      <div class="container is-max-desktop has-text-centered">
        <img src="static/images/overview_mmlu.Jpeg" alt="geometric reasoning">
        <p>Overview of the MMMU dataset. MMMU presents four challenges:
          1) <b>comprehensiveness</b>: 11.5K college-level problems across six broad disciplines and 30 college subjects;
          2) highly <b>heterogeneous</b> image types;
          3) <b>interleaved</b> text and images;
          4) <b>expert-level</b> perception and reasoning rooted in deep subject knowledge.
        </p>
      </div>
    </section>

    <section class="section">
      <div class="container" style="margin-bottom: 2vh;">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">üîîNews</h2>
            <div class="content has-text-justified">
              <p>
                <b>üî•[2024-09-22]: We release the new benchmark for text, image, and audio large language models!</b>
              </p>
          </div>
            <h2 class="title is-3">Introduction</h2>
            <div class="content has-text-justified">
              <p>
                Recent advancements in multimodal large language models (MLLMs) have aimed to integrate and interpret data across diverse modalities. However, the capacity of these models to concurrently process and reason about multiple modalities remains inadequately explored, partly due to the lack of comprehensive modality-wise benchmarks. We introduce OmniBench, a novel benchmark designed to rigorously evaluate models' ability to recognize, interpret, and reason across visual, acoustic, and textual inputs simultaneously. We define models capable of such tri-modal processing as omni-language models (OLMs).OmniBench is distinguished by high-quality human annotations, ensuring that accurate responses require integrated understanding and reasoning across all three modalities. Our main findings reveal that: i) open-source OLMs exhibit critical limitations in instruction-following and reasoning capabilities within tri-modal contexts; and ii) these baseline models perform poorly (below 50\% accuracy) even when provided with alternative textual representations of images and audio. These results suggest that the ability to construct a consistent context from text, image, and audio is often overlooked in existing MLLM training paradigms. We advocate for future research to focus on developing more robust tri-modal integration techniques and training strategies to enhance OLM performance across diverse modalities.
              </p>
            </div>
          </div>
        </div>
        <!--/ Abstract. -->
    </div>
    </section>

    <!-- DATASET SECTION -->
    <section class="hero is-light is-small">
      <div class="hero-body has-text-centered">
        <h1 class="title is-1 mmmu">
          <img src="static/images/mmmu_icon2.png" alt="Logo" class="mmmu-logo"/>
          <span class="mmmu">MMMU Benchmark</span>
        </h1>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Overview</h2>
            <div class="content has-text-justified">
              <p>
                We introduce the Massive Multi-discipline Multimodal Understanding and Reasoning (MMMU) benchmark, a novel benchmark meticulously curated to assess the expert-level multimodal understanding capability of foundation models across a broad scope of tasks. Covering subjects across disciplines, including Art, Business, Health & Medicine, Science, Humanities & Social Science, and Tech & Engineering, and over subfields. The detailed subject coverage and statistics are detailed in the figure. The questions in our benchmark were manually collected by a team of college students (including coauthors) from various disciplines and subjects, drawing from online sources, textbooks, and lecture materials.
              </p>
              <img src="static/images/mmlu_example.Jpeg" alt="algebraic reasoning" class="center">
              <br>
              <p>
                 MMMU is designed to measure three essential skills in LMMs: perception, knowledge, and reasoning. Our aim is to evaluate how well these models can not only perceive and understand information across different modalities but also apply reasoning with subject-specific knowledge to derive the solution.
              </p>
              <p>
                Our MMMU benchmark introduces key challenges to multimodal foundation models, as detailed in a figure. Among these, we particularly highlight the challenge stemming from the requirement for both expert-level visual perceptual abilities and deliberate reasoning with subject-specific knowledge. This challenge is vividly illustrated through our tasks, which not only demand the processing of various heterogeneous image types but also necessitate a model's adeptness in using domain-specific knowledge to deeply understand both the text and images and to reason. This goes significantly beyond basic visual perception, calling for an advanced approach that integrates advanced multimodal analysis with domain-specific knowledge.
              </p>
            </div>
          </div>
        </div>

        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Comparisons with Existing Benchmarks</h2>
            <div class="content has-text-justified">
              <p>
                To further distinguish the difference between <i>dataset</i> and other existing ones, we elaborate the benchmark details in Figure.
                From the <i>breadth</i> perspective, the prior benchmarks are heavily focused on daily knowledge and common sense.
                The covered image format is also limited. Our benchmark aims to cover college-level knowledge with 30 image formats including diagrams,
                tables, charts, chemical structures, photos, paintings, geometric shapes, music sheets, medical images, etc.
                In the <i>depth</i> aspect, the previous benchmarks normally require commonsense knowledge or simple physical or temporal reasoning.
                In contrast, our benchmark requires deliberate reasoning with college-level subject knowledge.
              </p>
              <div class="content has-text-centered">
                <img src="static/images/compare.Jpeg" alt="algebraic reasoning" class="center">
                <p> Sampled MMMU examples from each discipline. The questions and images need expert-level knowledge to understand and reason.</p>
              </div>
            </div>
          </div>
        </div>

        <div class="columns is-centered m-6">
          <div class="column is-full has-text-centered content">
            <h2 class="title is-3">Statistics</h2>
            <div class="carousel results-carousel">
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="static/images/mmmu_subject_distribution.Jpeg" alt="algebraic reasoning" width="95%"/>
                  <p> Sampled MMMU examples from each discipline. The questions and images need expert-level knowledge to understand and reason.</p>
                </div>
              </div>
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="static/images/statistics.png" alt="arithmetic reasoning" width="40%"/>
                  <p> Key statistics of the MMMU benchmark</p>
                </div>
              </div>
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="static/images/image_type_count.png" alt="arithmetic reasoning" width="80%"/>
                  <p> Distribution of image types in the MMMU dataset</p>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- RESULTS SECTION -->
    <section class="hero is-light is-small">
      <div class="hero-body has-text-centered">
        <h1 class="title is-1 mmmu">Experiment Results</h1>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <!-------------------------------------------------------------------- RESULTS SECTION -------------------------------------------------------------------->
        <div class="columns is-centered m-6">
          <div class="column is-full has-text-centered content">
            <h2 class="title is-3" id="leaderboard">Leaderboard</h2>
            <div class="content has-text-justified">
              <p>
                We evaluate various models including LLMs and LMMs.
                In each type, we consider both closed- and open-source models.
                Our evaluation is conducted under a zero-shot setting to assess the capability of models to generate accurate answers without fine-tuning or few-shot demonstrations on our benchmark.
                For all models, we use the default prompt provided by each model for multi-choice or open QA, if available.
                If models do not provide prompts for task types in MMMU, we conduct prompt engineering on the validation set and use the most effective prompt for the later zero-shot experiment.
              </p>
            </div>
            <br>
            <div class="model-labels-container">
              <span class="leaderboard-label human_expert">Human Expert</span>
              <span class="leaderboard-label open_source">Open-Source</span>
              <span class="leaderboard-label proprietary">Proprietary</span>
            </div>
            <br>
            <div class="content has-text-centered">
              <p>
                Click on MMMU-Pro, MMMU (Val) or MMMU (Test) to expand detailed results.
              </p>
            </div>
            <div class="leaderboard-container">
              <div class="table-wrapper">
                <table id="mmmu-table">
                  <thead>
                    <tr>
                      <th colspan="3" class="reset-cell clickable" style="text-align: center;">Reset</th>
                      <th class="pro-details-cell clickable" colspan="1">MMMU-Pro</th>
                      <th class="val-details-cell clickable" colspan="1">MMMU(Val)</th>
                      <th class="test-details-cell clickable" colspan="1">MMMU(Test)</th>
                    </tr>
                    <tr>
                      <th class="sortable clickable" data-sort="string">Name</th>
                      <th class="clickable" data-sort="string">Size</th>
                      <th class="sortable clickable" data-sort="date">Date</th>
                      <th class="sortable clickable pro-overall" data-sort="number">Overall</th>
                      <th class="hidden pro-details sortable clickable" data-sort="number">Vision</th>
                      <th class="hidden pro-details sortable clickable" data-sort="number">Standard</th>
                      <th class="sortable clickable val-overall" data-sort="number">Overall</th>
                      <th class="hidden val-details sortable clickable" data-sort="number">Art & Design</th>
                      <th class="hidden val-details sortable clickable" data-sort="number">Business</th>
                      <th class="hidden val-details sortable clickable" data-sort="number">Science</th>
                      <th class="hidden val-details sortable clickable" data-sort="number">Health & Medicine</th>
                      <th class="hidden val-details sortable clickable" data-sort="number">Human. & Social Sci.</th>
                      <th class="hidden val-details sortable clickable" data-sort="number">Tech & Eng.</th>
                      <th class="sortable clickable test-overall" data-sort="number">Overall</th>
                      <th class="hidden test-details sortable clickable" data-sort="number">Art & Design</th>
                      <th class="hidden test-details sortable clickable" data-sort="number">Business</th>
                      <th class="hidden test-details sortable clickable" data-sort="number">Science</th>
                      <th class="hidden test-details sortable clickable" data-sort="number">Health & Medicine</th>
                      <th class="hidden test-details sortable clickable" data-sort="number">Human. & Social Sci.</th>
                      <th class="hidden test-details sortable clickable" data-sort="number">Tech & Eng.</th>
                    </tr>
                  </thead>
                  <tbody>
                    <!-- Table body will be populated dynamically -->
                  </tbody>
                </table>
                <p class="test-desc"> Overall results of different models on the MMMU leaderboard. The best-performing model in each category is <b>in-bold</b>, and the second best is <u>underlined</u>. *: results provided by the authors.</p>
              </div>
            </div>
          </div>
        </div>
        <!-------------------------------------------------------------------- Image Type SECTION -------------------------------------------------------------------->
        <div class="columns is-centered m-6">
          <div class="column is-full has-text-centered content">
            <h2 class="title is-3">Different Image Types</h2>
            <div class="content has-text-justified">
              <p>
                We compare the performance of various models across top frequent image types.
                Across all types, GPT-4V consistently outperforms the other models by a huge margin.
                Open-source models demonstrate relatively strong performance in categories like Photos and Paintings, which are more frequently seen during training.
                However, for less common image categories like Geometric shapes, Music sheets and Chemical structures, all models obtain very low scores (some are close to random guesses).
                This indicates that the existing models are generalizing poorly towards these image types.
              </p>
            </div>
            <div class="model-labels-container">
              <span class="model-label" style="background-color: rgba(196, 123, 160, 0.5);">Adept Fuyu-8B</span>
              <span class="model-label" style="background-color: rgba(245, 123, 113, 0.5);">Qwen-VL-7B-Chat</span>
              <span class="model-label" style="background-color: rgba(255, 208, 80, 0.5);">LLaVA-1.5-13B</span>
              <span class="model-label" style="background-color: rgba(110, 194, 134, 0.5);">InstructBLIP-T5-XXL</span>
              <span class="model-label" style="background-color: rgba(255, 153, 78, 0.5);">BLIP-2 FLAN-T5-XXL</span>
              <span class="model-label" style="background-color: rgba(42, 149, 235, 0.5);">Yi-VL-34B</span>
              <span class="model-label" style="background-color: rgba(183, 156, 220, 0.5);">LLaVA-1.6-34B</span>
              <span class="model-label" style="background-color: rgba(143, 169, 209, 0.5);">InternVL-Chat-V1.2</span>
              <span class="model-label" style="background-color: rgba(72, 199, 176, 0.5);">VILA1.5</span>
              <span class="model-label" style="background-color: rgba(117, 209, 215, 0.5);">GPT-4V</span>
            </div>
            <div class="content has-text-centered">
              <div class="chart-grid">
                <!-- Chart 1: Diagrams -->
                <div class="chart-item">
                    <canvas id="chart_Diagrams"></canvas>
                    <p class="chart-label">Diagrams (3184)</p>
                </div>
                <!-- Chart 2: Tables -->
                <div class="chart-item">
                    <canvas id="chart_Tables"></canvas>
                    <p class="chart-label">Tables (2267)</p>
                </div>
                <!-- Chart 3: Plots and Charts -->
                <div class="chart-item">
                  <canvas id="chart_PlotsAndCharts"></canvas>
                  <p class="chart-label">Plots and Charts (840)</p>
                </div>
                <!-- Chart 4: Chemical Structures -->
                <div class="chart-item">
                  <canvas id="chart_ChemicalStructures"></canvas>
                  <p class="chart-label">Chemical Structures (573)</p>
                </div>
                <!-- Chart 5: Photographs -->
                <div class="chart-item">
                  <canvas id="chart_Photographs"></canvas>
                  <p class="chart-label">Photographs (770)</p>
                </div>
                <!-- Chart 6: Paintings -->
                <div class="chart-item">
                  <canvas id="chart_Paintings"></canvas>
                  <p class="chart-label">Paintings (453)</p>
                </div>
                <!-- Chart 7: Geometric Shapes -->
                <div class="chart-item">
                  <canvas id="chart_GeometricShapes"></canvas>
                  <p class="chart-label">Geometric Shapes (336)</p>
                </div>
                <!-- Chart 8: Sheet Music -->
                <div class="chart-item">
                  <canvas id="chart_SheetMusic"></canvas>
                  <p class="chart-label">Sheet Music (335)</p>
                </div>
                <!-- Chart 9: Medical Images -->
                <div class="chart-item">
                  <canvas id="chart_MedicalImages"></canvas>
                  <p class="chart-label">Medical Images (272)</p>
                </div>
                <!-- Chart 10: Pathological Images -->
                <div class="chart-item">
                  <canvas id="chart_PathologicalImages"></canvas>
                  <p class="chart-label">Pathological Images (253)</p>
                </div>
                <!-- Chart 11: Microscopic Images -->
                <div class="chart-item">
                  <canvas id="chart_MicroscopicImages"></canvas>
                  <p class="chart-label">Microscopic Images (226)</p>
                </div>
                <!-- Chart 12: MRI, CT scans, and X-rays -->
                <div class="chart-item">
                  <canvas id="chart_MRIsCTScansXrays"></canvas>
                  <p class="chart-label">MRI, CT scans, and X-rays (198)</p>
                </div>
                <!-- Chart 13: Sketches and Drafts -->
                <div class="chart-item">
                  <canvas id="chart_SketchesAndDrafts"></canvas>
                  <p class="chart-label">Sketches and Drafts (184)</p>
                </div>
                <!-- Chart 14: Maps -->
                <div class="chart-item">
                  <canvas id="chart_Maps"></canvas>
                  <p class="chart-label">Maps (170)</p>
                </div>
                <!-- Chart 15: Technical Blueprints -->
                <div class="chart-item">
                  <canvas id="chart_TechnicalBlueprints"></canvas>
                  <p class="chart-label">Technical Blueprints (162)</p>
                </div>
                <!-- Chart 16: Trees and Graphs -->
                <div class="chart-item">
                  <canvas id="chart_TreesAndGraphs"></canvas>
                  <p class="chart-label">Trees and Graphs (146)</p>
                </div>
                <!-- Chart 17: Mathematical Notations -->
                <div class="chart-item">
                  <canvas id="chart_MathematicalNotations"></canvas>
                  <p class="chart-label">Mathematical Notations (133)</p>
                </div>
                <!-- Chart 18: Comics and Cartoons -->
                <div class="chart-item">
                  <canvas id="chart_ComicsAndCartoons"></canvas>
                  <p class="chart-label">Comics and Cartoons (131)</p>
                </div>
                <!-- Chart 19: Sculpture -->
                <div class="chart-item">
                  <canvas id="chart_Sculpture"></canvas>
                  <p class="chart-label">Sculpture (117)</p>
                </div>
                <!-- Chart 20: Portraits -->
                <div class="chart-item">
                  <canvas id="chart_Portraits"></canvas>
                  <p class="chart-label">Portraits (91)</p>
                </div>
                <!-- Chart 21: Screenshots -->
                <div class="chart-item">
                  <canvas id="chart_Screenshots"></canvas>
                  <p class="chart-label">Screenshots (70)</p>
                </div>
                <!-- Chart 22: Other -->
                <div class="chart-item">
                  <canvas id="chart_Other"></canvas>
                  <p class="chart-label">Other(60)</p>
                </div>
                <!-- Chart 23: Poster -->
                <div class="chart-item">
                  <canvas id="chart_Poster"></canvas>
                  <p class="chart-label">Poster(57)</p>
                </div>
                <!-- Chart 24: Icons and Symbols -->
                <div class="chart-item">
                  <canvas id="chart_IconsAndSymbols"></canvas>
                  <p class="chart-label">Icons and Symbols (42)</p>
                </div>
                <!-- Chart 25: Historical Timelines -->
                <div class="chart-item">
                  <canvas id="chart_HistoricalTimelines"></canvas>
                  <p class="chart-label">Historical Timelines (30)</p>
                </div>
                <!-- Chart 26: 3D Renderings -->
                <div class="chart-item">
                  <canvas id="chart_3DRenderings"></canvas>
                  <p class="chart-label">3D Renderings (21)</p>
                </div>
                <!-- Chart 27: DNA Sequences -->
                <div class="chart-item">
                  <canvas id="chart_DNASequences"></canvas>
                  <p class="chart-label">DNA Sequences (20)</p>
                </div>
                <!-- Chart 28: Landscapes -->
                <div class="chart-item">
                  <canvas id="chart_Landscapes"></canvas>
                  <p class="chart-label">Landscapes (16)</p>
                </div>
                <!-- Chart 29: Logos and Branding -->
                <div class="chart-item">
                  <canvas id="chart_LogosAndBranding"></canvas>
                  <p class="chart-label">Logos and Branding(14)</p>
                </div>
                <!-- Chart 30: Advertisements -->
                <div class="chart-item">
                  <canvas id="chart_Advertisements"></canvas>
                  <p class="chart-label">Advertisements (10)</p>
                </div>
              </div>
              <p class="bottom-text"> Selected models' performance on 30 different image types. Note that a single image may have multiple image types.</p>
            </div>
          </div>
        </div>
        <!-------------------------------------------------------------------- Difficulty Levels SECTION -------------------------------------------------------------------->
        <div class="columns is-centered m-6">
          <div class="column is-full has-text-centered content">
            <h2 class="title is-3">Different Difficulty Levels</h2>
            <div class="content has-text-justified">
              <p>
                we compares the performance of selected models across three difficulty levels.
                GPT-4V demonstrates a significantly higher proficiency, with a success rate of 76.1%, compared to opensource models in the ‚ÄúEasy‚Äù category.
                When it comes to the ‚ÄúMedium‚Äù category, while the gap narrows, GPT-4V still leads at 55.6%.
                The further diminishing performance gap in the ‚ÄúHard‚Äù category across models indicates that as the complexity of tasks increases, the advantage of more advanced models like GPT-4V almost disappears.
                This might reflect a current limitation in handling expert-level challenging queries even for the most advanced models.
              </p>
            </div>
            <div class="content has-text-centered">
              <p>Click legend to switch the comparison chart.</p>
            </div>
            <div class="content has-text-centered">
              <canvas id="difficulty_level_chart"></canvas>
              <p>Result decomposition across question difficulty levels.</p>
            </div>
          </div>
        </div>
      <!-------------------------------------------------------------------- Single VS Multiple image SECTION -------------------------------------------------------------------->
        <div class="columns is-centered m-6">
        <div class="column is-full has-text-centered content">
          <h2 class="title is-3">Single Image VS Multiple Image</h2>
          <div class="content has-text-centered">
            <div class="content has-text-centered">
              <p>Click legend to switch the comparison chart.</p>
            </div>
            <canvas id="single_vs_multiple_chart"></canvas>
            <p>Result decomposition across single image and multiple image tasks.</p>
          </div>
        </div>
        </div>
      <!-------------------------------------------------------------------- Error Analysis SECTION -------------------------------------------------------------------->
        <div class="columns is-centered m-6">
          <div class="column is-full has-text-centered content">
            <h2 class="title is-3">Error Analysis</h2>
            <div class="content has-text-justified">
              <p>
                We delve into the analysis of errors by GPT-4V, a pivotal aspect for understanding its operational capabilities and limitations.
                This analysis serves not only to identify the model's current shortcomings but also to guide future enhancements in its design and training.
                We meticulously examine 150 randomly sampled error instances from GPT-4V's predictions.
                These instances are analyzed by expert annotators who identify the root causes of mispredictions based on their knowledge and the golden explanations if available.
                The distribution of these errors is illustrated in Figure, and a selection of 100 notable cases, along with detailed analyses, is included in the Appendix.
              </p>
            </div>
            <div class="content has-text-centered">
              <img src="static/images/error_distribution_1.Jpeg" alt="error distribution" width="25%">
              <p> Error distribution over 150 annotated GPT-4V errors.</p>
            </div>
          </div>
        </div>
      <!-------------------------------------------------------------------- Error Example  -------------------------------------------------------------------->
        <div class="columns is-centered m-6">
          <div class="column is-full has-text-centered content">
            <h2 class="title is-3" id="examples">Error Examples</h2>
            <div class="carousel results-carousel">
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="static/images/error/1.png" alt="grade-lv" width="60%"/>
                </div>
              </div>
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="static/images/error/2.png" alt="grade-lv" width="60%"/>
                </div>
              </div>
            </div>
          </div>
        </div>
      <!-------------------------------------------------------------------- Correct Example -------------------------------------------------------------------->
        <div class="columns is-centered m-6">
          <div class="column is-full has-text-centered content">
            <h2 class="title is-3">Correct Examples</h2>
            <div id="results-carousel" class="carousel results-carousel">
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="static/images/correct/1.png" alt="grade-lv" width="60%"/>
                </div>
              </div>
              <div class="box m-5">
                <div class="content has-text-centered">
                  <img src="static/images/correct/2.png" alt="grade-lv" width="60%"/>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- @PAN TODO: bibtex -->
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title is-3 has-text-centered">BibTeX</h2>
        <pre><code>
          @inproceedings{yue2023mmmu,
            title={MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI},
            author={Xiang Yue and Yuansheng Ni and Kai Zhang and Tianyu Zheng and Ruoqi Liu and Ge Zhang and Samuel Stevens and Dongfu Jiang and Weiming Ren and Yuxuan Sun and Cong Wei and Botao Yu and Ruibin Yuan and Renliang Sun and Ming Yin and Boyuan Zheng and Zhenzhu Yang and Yibo Liu and Wenhao Huang and Huan Sun and Yu Su and Wenhu Chen},
            booktitle={Proceedings of CVPR},
            year={2024},
          }
    </code></pre>
      </div>
    </section>

    <footer class="footer">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is website adapted from <a href="https://nerfies.github.io/">Nerfies</a> and <a href="https://mathvista.github.io/">MathVista</a>, licensed under a <a rel="license"
                                                  href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
          </div>
        </div>
      </div>
    </footer>

  </body>
</html>
